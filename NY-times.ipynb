{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url definition\n",
    "url = \"https://www.nytimes.com/section/world\"\n",
    "\n",
    "# Request\n",
    "r1 = requests.get(url)\n",
    "r1.status_code\n",
    "\n",
    "# We'll save in coverpage the cover page content\n",
    "coverpage = r1.content\n",
    "\n",
    "# Soup creation\n",
    "soup1 = BeautifulSoup(coverpage, 'lxml') #parser\n",
    "\n",
    "# News identification\n",
    "coverpage_news = soup1.find_all('div', class_=\"css-4svvz1 ekkqrpp0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ol = coverpage_news[0].find_all('ol', class_=\"css-11jjg ekkqrpp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists for img-url, titles, Category and content \n",
    "img_lst = []\n",
    "title_lst = []\n",
    "content_lst = []\n",
    "\n",
    "for n in [0, 2, 3]:\n",
    "    # Getting img url\n",
    "    try:\n",
    "        img = ol[n].find_all('img')\n",
    "        for i in img:\n",
    "            img_lst.append(i['src'])\n",
    "    except:\n",
    "        img_lst.append('Null')\n",
    "        \n",
    "        \n",
    "    # Extracting h2 tag\n",
    "    if n == 0:\n",
    "        tag = ol[n].find_all('h2', class_=\"css-l2vidh e4e4i5l1\")\n",
    "    else:\n",
    "        try:\n",
    "            tag = ol[n].find_all('h2', class_=\"css-y3otqb e134j7ei0\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    for i in tag:\n",
    "        # Getting title\n",
    "        title = i.find('a').get_text()\n",
    "        title_lst.append(title)\n",
    "        \n",
    "        # Getting the link of the article\n",
    "        link = \"https://www.nytimes.com/\" + i.find('a')['href']\n",
    "        \n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        try:\n",
    "            article = requests.get(link)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            requests.status_code = \"Connection refused\"\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'lxml')\n",
    "        body = soup_article.find_all('section', class_='meteredContent css-1r7ky0e')\n",
    "        try:\n",
    "            x = body[0].find_all('p')\n",
    "            # Unifying the paragraphs\n",
    "            list_paragraphs = []\n",
    "            for p in np.arange(0, len(x)):\n",
    "                paragraph = x[p].get_text()\n",
    "                list_paragraphs.append(paragraph)\n",
    "                final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "            # Removing special characters\n",
    "            final_article = re.sub(\"\\\\xa0\", \"\", final_article)\n",
    "            content_lst.append(final_article)\n",
    "        except IndexError:\n",
    "            content_lst.append(\"Null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning repeated photos\n",
    "for i in [1, 2, 3, 4]:\n",
    "    img_lst.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst2 = []\n",
    "for title in title_lst:\n",
    "    # Define stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenize and tag some text:\n",
    "    words = word_tokenize(title)\n",
    "\n",
    "    filtered_sentence = [w for w in words if not w in stop_words] # Delete extra words\n",
    "\n",
    "    pos_tag = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "    code = []\n",
    "    for i in pos_tag:\n",
    "        if i[1] == 'NNP' or i[1] == 'JJ' or i[1] == 'NNPS' or i[1] == 'VBP' or i[1] == 'NN':\n",
    "            code.append(i[0])\n",
    "\n",
    "    if 'COVID-19' in code:\n",
    "        code.remove('COVID-19')\n",
    "\n",
    "    # Identify named entities:\n",
    "    nes = nltk.ne_chunk(pos_tag)\n",
    "\n",
    "    name = []\n",
    "    for ne in nes:\n",
    "        if type(ne) is nltk.tree.Tree:\n",
    "            if ne.label() in ['GPE', 'LOCATION']:\n",
    "                name.append(u' '.join([i[0] for i in ne.leaves()]))\n",
    "\n",
    "    lst = list(set(name + code))\n",
    "\n",
    "    with open(\"countries.json\") as f:\n",
    "        countries = json.load(f)\n",
    "        \n",
    "    lst1 = []\n",
    "    for i in lst:\n",
    "        for country in countries:\n",
    "            if re.search(country['name'], i) or re.search(country['code'], i):\n",
    "                lst1.append(country['code'])\n",
    "        else:\n",
    "            lst1.append(\"Null\")\n",
    "    lst2.append(lst1)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_lst = []\n",
    "for lst in lst2:\n",
    "    lst = list(set(lst))\n",
    "    if len(lst) == 1:\n",
    "        location_lst.append('World')\n",
    "    else:\n",
    "        for i in lst:\n",
    "            if i != \"Null\":\n",
    "                location_lst.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'World',\n",
       " 'NI',\n",
       " 'World',\n",
       " 'World',\n",
       " 'UK',\n",
       " 'CN',\n",
       " 'NI',\n",
       " 'World',\n",
       " 'World',\n",
       " 'UK',\n",
       " 'CN']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_lst            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ext_env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator SVC from version 0.19.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "with open('best_svc.pickle', 'rb') as data: #sklearn\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ext_env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "d:\\ext_env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF object\n",
    "with open('tfidf.pickle', 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category mapping dictionary\n",
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4,\n",
    "    'other':5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_show_info\n",
    "df_show_info = pd.DataFrame(\n",
    "        {'Newspaper': 'The New York Times',\n",
    "        'Title': title_lst,\n",
    "        'Img': img_lst,\n",
    "        'Content': content_lst,\n",
    "        'Location': location_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Title</th>\n",
       "      <th>Img</th>\n",
       "      <th>Content</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>‘Our Role Is to Reduce Their Grief’</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>THE DESERT WEST OF NAJAF — There are no signs ...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Coronavirus Live Updates: Trump Administration...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>The Trump administration has balked at providi...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>As Seasonal Rains Fall, Dispute Over Nile Dam ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/16/us/...</td>\n",
       "      <td>CAIRO — Every day now, seasonal rain pounds th...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>London Police Urged to Apologize After Officer...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/16/us/...</td>\n",
       "      <td>LONDON — The lawyer for a Black man who repeat...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Nicaragua’s Ruling Sandinistas Fall Victim to ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>A string of recent deaths across Nicaragua — i...</td>\n",
       "      <td>NI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Defying Kremlin, Protesters Stage Biggest Rall...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>MOSCOW — Ignoring pleas from the Kremlin for c...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Fire Hits Cathedral in French City of Nantes</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/18/wor...</td>\n",
       "      <td>A fire broke out inside the cathedral of the w...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Long Waits for U.K. Hospital Treatment as N.H....</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/18/wor...</td>\n",
       "      <td>LONDON — After nine months of waiting for surg...</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>China’s Swimwear Capital Can’t Wait for You to...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>There may be no place on earth that had been l...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Nicaragua’s Ruling Sandinistas Fall Victim to ...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>A string of recent deaths across Nicaragua — i...</td>\n",
       "      <td>NI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Defying Kremlin, Protesters Stage Biggest Rall...</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>MOSCOW — Ignoring pleas from the Kremlin for c...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Fire Hits Cathedral in French City of Nantes</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/19/wor...</td>\n",
       "      <td>A fire broke out inside the cathedral of the w...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Long Waits for U.K. Hospital Treatment as N.H....</td>\n",
       "      <td>https://static01.nyt.com/images/2020/07/16/bus...</td>\n",
       "      <td>LONDON — After nine months of waiting for surg...</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>China’s Swimwear Capital Can’t Wait for You to...</td>\n",
       "      <td>Null</td>\n",
       "      <td>There may be no place on earth that had been l...</td>\n",
       "      <td>CN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Newspaper                                              Title  \\\n",
       "0   The New York Times                ‘Our Role Is to Reduce Their Grief’   \n",
       "1   The New York Times  Coronavirus Live Updates: Trump Administration...   \n",
       "2   The New York Times  As Seasonal Rains Fall, Dispute Over Nile Dam ...   \n",
       "3   The New York Times  London Police Urged to Apologize After Officer...   \n",
       "4   The New York Times  Nicaragua’s Ruling Sandinistas Fall Victim to ...   \n",
       "5   The New York Times  Defying Kremlin, Protesters Stage Biggest Rall...   \n",
       "6   The New York Times       Fire Hits Cathedral in French City of Nantes   \n",
       "7   The New York Times  Long Waits for U.K. Hospital Treatment as N.H....   \n",
       "8   The New York Times  China’s Swimwear Capital Can’t Wait for You to...   \n",
       "9   The New York Times  Nicaragua’s Ruling Sandinistas Fall Victim to ...   \n",
       "10  The New York Times  Defying Kremlin, Protesters Stage Biggest Rall...   \n",
       "11  The New York Times       Fire Hits Cathedral in French City of Nantes   \n",
       "12  The New York Times  Long Waits for U.K. Hospital Treatment as N.H....   \n",
       "13  The New York Times  China’s Swimwear Capital Can’t Wait for You to...   \n",
       "\n",
       "                                                  Img  \\\n",
       "0   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "1   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "2   https://static01.nyt.com/images/2020/07/16/us/...   \n",
       "3   https://static01.nyt.com/images/2020/07/16/us/...   \n",
       "4   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "5   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "6   https://static01.nyt.com/images/2020/07/18/wor...   \n",
       "7   https://static01.nyt.com/images/2020/07/18/wor...   \n",
       "8   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "9   https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "10  https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "11  https://static01.nyt.com/images/2020/07/19/wor...   \n",
       "12  https://static01.nyt.com/images/2020/07/16/bus...   \n",
       "13                                               Null   \n",
       "\n",
       "                                              Content Location  \n",
       "0   THE DESERT WEST OF NAJAF — There are no signs ...    World  \n",
       "1   The Trump administration has balked at providi...    World  \n",
       "2   CAIRO — Every day now, seasonal rain pounds th...    World  \n",
       "3   LONDON — The lawyer for a Black man who repeat...    World  \n",
       "4   A string of recent deaths across Nicaragua — i...       NI  \n",
       "5   MOSCOW — Ignoring pleas from the Kremlin for c...    World  \n",
       "6   A fire broke out inside the cathedral of the w...    World  \n",
       "7   LONDON — After nine months of waiting for surg...       UK  \n",
       "8   There may be no place on earth that had been l...       CN  \n",
       "9   A string of recent deaths across Nicaragua — i...       NI  \n",
       "10  MOSCOW — Ignoring pleas from the Kremlin for c...    World  \n",
       "11  A fire broke out inside the cathedral of the w...    World  \n",
       "12  LONDON — After nine months of waiting for surg...       UK  \n",
       "13  There may be no place on earth that had been l...       CN  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering Functions\n",
    "\n",
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    #df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'}, inplace = True)\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    features = features.reshape(-1, 1)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Functions\n",
    "\n",
    "def predict_from_features(features):\n",
    "        \n",
    "    # Obtain the highest probability of the predictions for each article\n",
    "    svc = SVC(probability=True)\n",
    "    X_train, y_train = train_test_split(features,\n",
    "                                        test_size=0.25,)\n",
    "    svc.fit(X_train, y_train.values.ravel())\n",
    "    predictions_proba = svc.predict_proba(features).max(axis=1)\n",
    "    #predictions_proba = svc_model.predict_proba(features).max(axis=1)    \n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions_pre = svc_model.predict(features)\n",
    "\n",
    "    # Replace prediction with 6 if associated cond. probability less than threshold\n",
    "    predictions = []\n",
    "\n",
    "    for prob, cat in zip(predictions_proba, predictions_pre):\n",
    "        if prob > .65:\n",
    "            predictions.append(cat)\n",
    "        else:\n",
    "            predictions.append(5)\n",
    "\n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "features = create_features_from_df(df_show_info)\n",
    "\n",
    "# Predict\n",
    "predictions = predict_from_features(features)\n",
    "\n",
    "# Put into dataset\n",
    "df = complete_df(df_show_info['Category'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_json('nytimes19.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
